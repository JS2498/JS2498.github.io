<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <head>
       <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] , ['$$', '$$']],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    </head>

    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jayanth  S | Monte-Carlo Algorithm with Reward Reshaping for Mountain Car gym environment</title>
    <meta name="author" content="Jayanth  S" />
    <meta name="description" content="This blog post explains the On-policy Every-Visit Monte-Carlo algorithm and its implementation in *MountainCar-v0* openai-gym environment." />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://js2498.github.io/blog/2022/code/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    
  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://js2498.github.io/"><span class="font-weight-bold">Jayanth</span>   S</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Monte-Carlo Algorithm with Reward Reshaping for Mountain Car gym environment</h1>
    <p class="post-meta">April 25, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/ReinforcementLearning">
          <i class="fas fa-hashtag fa-sm"></i> ReinforcementLearning</a>  
          
        ·  
        <a href="/blog/category/rl-posts">
          <i class="fas fa-tag fa-sm"></i> rl-posts</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>This blog post explains the On-policy Every-Visit Monte-Carlo algorithm and its implementation in <em>MountainCar-v0</em> openai-gym environment. It is assumed that the reader has a basic understanding of Markov Decision Process (MDP), value iteration and policy iteration. Refer to <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/" target="_blank" rel="noopener noreferrer">Long-peak into RL- Lilian Weng</a>, which explains all the basic concepts of RL.</p>

<p>For any value-based reinforcement learning (RL) algorithm, <strong><em>prediction (policy evaluation)</em></strong> and <strong><em>control (policy improvement)</em></strong> are the two main steps. Policy evaluation corresponds to finding the state-value for a given policy. The state-value corresponding to state \(s\) for a policy \(\pi\) is given by,</p>

\[v_{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=t_0}^T \gamma^{(t-t_0)} r_t|(s_{t_0}=s)\right]\]

<p>where \(T\) is the episode length and \(\gamma \in (0,1]\) is the discount factor. In Monte-Carlo method, the state-value \(v_{\pi}(s)\) is estimated by averaging the returns over episodes. The return for state \(s\) vistited at time step \(t_0\) while following the policy \(\pi\) is given by,</p>

\[G(s)|_{(s_{t_0} = s)} = \sum_{t=t_0}^T \gamma^{(t-t_0)} r_t\]

<p>In MC method the state-value is estimated as,</p>

\[\hat{v}_{\pi}(s) = \frac{\sum_{n=1}^{N}G_i(s)}{N}\]

<p>Since a state \(s\) can be visited multiple times in a episode, the estimation of state-value function can be either performed using <strong>every-visit Monte Carlo method</strong> or <strong>first-visit Monte Carlo method</strong>. Hence, in the above equation \(N\) is the number of episodes in which the state is visited at least once in first-visit MC method and in every-visit MC method, \(N\) represents the number of times a state is visited including the possible multiple visits in a single episode. Here, \(G_i(s)\) is the return obtained corresponding to the \(i^{\rm th}\) visit to the state \(s\). Following policy \(\pi\) will only help us to estimate the state-value corresponding to policy \(\pi\) and doesn’t improve the policy. To improve the policy we need to try different policies i.e., <strong>exploration</strong> has to be done. Hence we use \(\epsilon\)-soft policy for Monte-Carlo control. In \(\epsilon\)-soft policy, for a state \(s\) the greedy action (action which gives the maximum state-action value) is chosen with probability \(\left(1-\epsilon + \frac{\epsilon}{N_a}\right)\) and any other action is chosen with probability given by \(\frac{\epsilon}{N_a}\), where \(N_a\) is the number of different actions that can be chosen in a state \(s\).</p>

<p>We run the Monte-Carlo algorithm on the <em>MountainCar-v0</em> openai-gym environment. The description of the environment is described as follows:</p>

<p><strong>Environment Description</strong>:
    A car has to travel in a one-dimensional (horizontal) track with the road
    being similar to a valley between mountains on either side of it. The goal
    is to reach the top of right side mountain starting in the valley.</p>

<p><strong>State space</strong>: 2-D states representing (position, velocity)</p>

<p><strong>Action space</strong>:</p>

<ol>
  <li>Push left (accelerate left),</li>
  <li>Push right (accelerate right),</li>
  <li>Do nothing</li>
</ol>

<p><strong>Reward</strong>:</p>

<ul>
  <li>Reward \(=-1\) for any action taken.</li>
  <li>Reward \(=0\) on reaching the goal.</li>
</ul>

<p><strong>Initial state</strong>: \((x,0)\) with \(x\) taking value between \([-0.6,-0.4]\) uniformally.</p>

<p><strong>Termination</strong>:</p>

<ul>
  <li>On reaching the goal i.e. \(x&gt;0.5\).</li>
  <li>Else if length of episode is \(200\).</li>
</ul>

<p><strong>Remember</strong>: \(x \in [-1.2, 0.6]\) and the max speed is \(0.07\) with the state space being continuous.</p>

<p>From the environment description we can observe that the state space of <em>MountainCar-v0</em> environment is continuous, whereas the Monte-Carlo RL methods are tabular methods which stores the state-action values \(Q(s,a)\) for each state and action pair belonging to the finite-dimensional state and action space. Hence we use the following function to discretize the observations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">discretize_obs</span><span class="p">(</span><span class="n">continuous_state</span><span class="p">):</span>
    <span class="n">pos_precision</span> <span class="o">=</span> <span class="mf">0.09</span>
    <span class="n">vel_precision</span> <span class="o">=</span> <span class="mf">0.0007</span>

    <span class="n">min_pos</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.2</span>
    <span class="n">min_vel</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.07</span>

    <span class="c1"># max_pos = 0.6
</span>    <span class="c1"># max_vel = 0.07
</span>
    <span class="n">dis_pos</span> <span class="o">=</span> <span class="nf">int</span><span class="p">((</span><span class="n">continuous_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">min_pos</span><span class="p">)</span> <span class="o">/</span> <span class="n">pos_precision</span><span class="p">)</span>
    <span class="n">dis_vel</span> <span class="o">=</span> <span class="nf">int</span><span class="p">((</span><span class="n">continuous_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">min_vel</span><span class="p">)</span> <span class="o">/</span> <span class="n">vel_precision</span><span class="p">)</span>

    <span class="c1"># print(dis_pos, dis_vel)
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">dis_pos</span><span class="p">,</span> <span class="n">dis_vel</span><span class="p">])</span>
</code></pre></div></div>

<p>The code for First-visit Monte Carlo method is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MonteCarlo</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_actions</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps_reduction</span> <span class="o">=</span> <span class="mf">0.99999</span>  <span class="c1">#(self.epsilon - self.epsilon_min)/400000
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n_actions</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_actions</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">self</span><span class="p">.</span><span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_actions</span><span class="p">))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>

    <span class="k">def</span> <span class="nf">update_Qvalues</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">states_visited</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="n">states_visited</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">episode</span><span class="p">[</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>

        <span class="c1"># print(states_visited)
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="c1"># print(i,T)
</span>            <span class="n">state</span> <span class="o">=</span> <span class="n">episode</span><span class="p">[</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">episode</span><span class="p">[</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">episode</span><span class="p">[</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
            <span class="c1"># print(list(state))
</span>

            <span class="n">G</span> <span class="o">=</span>  <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span> <span class="o">+</span> <span class="n">reward</span>

            <span class="k">if</span> <span class="nf">list</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">states_visited</span><span class="p">[:</span><span class="n">T</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="n">i</span><span class="p">]</span> <span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">count</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span>   <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">count</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span>
        <span class="c1"># print(G)
</span>
    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">action_space</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])</span>
        <span class="k">return</span> <span class="n">action</span>
</code></pre></div></div>

<p>The First-Vist MC agent as described above was not able to learn a optimal policy to reach the destination even after playing \(10^{6}\) games/episodes. Increasing the number of timesteps in a episode to \(500\) or \(800\) also didn’t work. This may be due to the fact that the agent only receives \(-1\) as the reward till it reaches the destination. Hence the agent is not able to find a policy that would reslut in reaching the destination. To overcome this, an additional reward was given whenever the agent took an action that resulted in the position of next state being greater than \(0.05\). Since the car has to move back and forth to reach the destination, the actions that resulted in car position being less than $0.05$ were not penalized. An extra reward of \(100\) was given whenever the agent reached the destination. It was observed that under this reward reshaping the agent was able to find a optimal policy resulting in the car reaching the destination. Concretely, the changes made to the reward structure was:</p>

<ul>
  <li>If the position in next state is greater than \(0.05\), then a additional reward of \(\exp({\text {position of next state}\times5})\) was given. (There is no need to multiply position of next state by \(5\). It works for any other value \(&gt;1\))</li>
  <li>If it reaches the destination then a extra reward of \(100\) was given.</li>
  <li>The maximum number of steps in a episode was increased to \(500\).</li>
</ul>

<p>The corresponding code for training the agent is given below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">MountainCar-v0</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># specify the environment you wish to have
</span><span class="n">env</span><span class="p">.</span><span class="n">_max_episodes_steps</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">env</span><span class="p">.</span><span class="n">_max_episode_steps</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># learning rate
</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="c1"># discount factor
</span><span class="n">n_games</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Initial exploration factor
</span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">avg_scores</span> <span class="o">=</span> <span class="p">[]</span>


<span class="n">num_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">high</span> <span class="o">-</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">low</span><span class="p">)</span><span class="o">/</span><span class="p">([</span><span class="n">pos_precision</span><span class="p">,</span> <span class="n">vel_precision</span><span class="p">])</span>
<span class="n">num_states</span> <span class="o">=</span> <span class="n">num_states</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="nc">MonteCarlo</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">n_actions</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">num_states</span><span class="o">=</span><span class="n">num_states</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_games</span><span class="p">):</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>  <span class="c1"># initalizes the starting state of the car
</span>    <span class="n">discrete_obs</span> <span class="o">=</span> <span class="nf">discretize_obs</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">episode</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">5001</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print(f"Episode is {i}")
</span>            <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">choose_action</span><span class="p">(</span><span class="n">discrete_obs</span><span class="p">)</span>
        <span class="n">next_observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">discrete_next</span> <span class="o">=</span> <span class="nf">discretize_obs</span><span class="p">(</span><span class="n">next_observation</span><span class="p">)</span>
        <span class="c1"># print(discrete_next, reward, action)
</span>
        <span class="k">if</span> <span class="n">next_observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">next_observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">+=</span> <span class="mi">100</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="n">episode</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">discrete_obs</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>

        <span class="n">discrete_obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">copy</span><span class="p">(</span><span class="n">discrete_next</span><span class="p">)</span>

    <span class="n">agent</span><span class="p">.</span><span class="nf">update_Qvalues</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">((</span><span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="o">*</span><span class="n">agent</span><span class="p">.</span><span class="n">eps_reduction</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">epsilon_min</span><span class="p">))</span>
    <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="n">avg_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

    <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The average reward is </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="si">:</span><span class="p">])</span><span class="si">}</span><span class="s"> and epsiode is </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>



<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_games</span><span class="p">)]</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">avg_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epsiodes</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Score</span><span class="sh">"</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<p>Once the agent was trained, it was run for a specifice number of episodes and most of the times the car reached the destination. The corresponding code is given as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_games</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_games</span><span class="p">):</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>  <span class="c1"># initalizes the starting state of the car
</span>    <span class="n">discrete_obs</span> <span class="o">=</span> <span class="nf">discretize_obs</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">episodes_length</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">episode_length</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># episode = []
</span>    <span class="n">avg_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
        
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">choose_action</span><span class="p">(</span><span class="n">discrete_obs</span><span class="p">)</span>
        <span class="n">next_observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">discrete_next</span> <span class="o">=</span> <span class="nf">discretize_obs</span><span class="p">(</span><span class="n">next_observation</span><span class="p">)</span>
        <span class="c1"># print(discrete_next, reward, action)
</span>
        <span class="k">if</span> <span class="n">next_observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">next_observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">+=</span> <span class="mi">100</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="n">discrete_obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">copy</span><span class="p">(</span><span class="n">discrete_next</span><span class="p">)</span>
        <span class="n">episode_length</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">episodes_length</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">episode_length</span><span class="p">)</span>

    <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="n">avg_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]))</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The epsiode is </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s"> with episode length </span><span class="si">{</span><span class="n">episode_length</span><span class="si">}</span><span class="s"> and total reward </span><span class="si">{</span><span class="n">score</span><span class="si">:</span> <span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_games</span><span class="p">)]</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">avg_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epsiodes</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Score</span><span class="sh">"</span><span class="p">)</span>

<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<p>One episode of the agent playing in the <em>MountainCar-v0</em> environment is given below (In the .gif <em>Episode</em> refers to the time-step):</p>

<div class="col-sm mt-3">
    <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/MonteCarlo_EveryVisit_MountainCar.gif-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/MonteCarlo_EveryVisit_MountainCar.gif-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/MonteCarlo_EveryVisit_MountainCar.gif-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/MonteCarlo_EveryVisit_MountainCar.gif" title="Monte-Carlo agent in Mountain Car environment">

  </picture>

</figure>

</div>

<p>The complete code of first-vist MC agent for <em>MountainCar-v0</em> can be found <a href="https://github.com/JS2498/CS420-Reinforcement-Learning/blob/main/Assignment_4/OnPolicy_MC_MountainCar.py" target="_blank" rel="noopener noreferrer">here.</a></p>

  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2024 Jayanth  S. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

